{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# from thefuzz import fuzz, process\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fpl_players(fpl_subfolders, season):\n",
    "\n",
    "    for folder in fpl_subfolders:\n",
    "        # print(folder)\n",
    "        # print('flp: ', folder)\n",
    "        player_name = folder.split('/')[6].split('_')\n",
    "        clean_player_name = \" \".join(player_name[0:-1])\n",
    "        player_df = pd.read_csv(str(folder+'/gw.csv'))\n",
    "        player_dir = './data/joint/'+str(season)+'/fpl/'\n",
    "         # Check if player_dir exists\n",
    "\n",
    "\n",
    "        if not os.path.exists(player_dir):\n",
    "            os.makedirs(player_dir)\n",
    "        player_df.to_csv(player_dir + str(clean_player_name) + '.csv', index_label=False)\n",
    "\n",
    "    print('successfully cleaned 20'+ season +' fpl data')\n",
    "\n",
    "# Understat Files\n",
    "def save_under_players(understat_files, season):\n",
    "    for file_ in understat_files:\n",
    "        player_name = file_.split('/')[6].split('_')\n",
    "        clean_player_name = \" \".join(player_name[0:-1])\n",
    "        player_df = pd.read_csv(str(file_))\n",
    "\n",
    "        player_dir = './data/joint/'+str(season)+'/understat/'\n",
    "        # Check if player_dir exists\n",
    "        if not os.path.exists(player_dir):\n",
    "            os.makedirs(player_dir)\n",
    "        player_df.to_csv(player_dir + str(clean_player_name) + '.csv', index_label=False)\n",
    "\n",
    "    print('successfully cleaned understat 20'+ season +' data')\n",
    "\n",
    "def joint_players_info(fpl_player_folder_path, understat_player_folder_path, season):\n",
    "    fpl_subfolders = [ f.path for f in os.scandir(fpl_player_folder_path) if f.is_dir() ]\n",
    "    under_files = [ f.path for f in os.scandir(understat_player_folder_path) if f.is_file() ]\n",
    "\n",
    "    save_fpl_players(fpl_subfolders, season)\n",
    "    save_under_players(under_files, season)\n",
    "\n",
    "    print('20'+ season +' fpl and understat data now in `joint` folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_fpl_understat_data(fpl_player_folder_path, understat_player_folder_path, season):\n",
    "    joint_players_info(fpl_player_folder_path, understat_player_folder_path, season)\n",
    "\n",
    "    joint_fpl_data_path = \"./data/joint/\"+ season + \"/fpl/\"\n",
    "    joint_understat_path = \"./data/joint/\"+ season + \"/understat/\"\n",
    "\n",
    "    understat_files= next(os.walk(\"./data/joint/\"+ season + \"/understat/\"), (None, None, []))[2]  # [] if no file\n",
    "    fpl_files = next(os.walk( \"./data/joint/\"+ season +\"/fpl\"), (None, None, []))[2]  # [] if no file\n",
    "    player_ids = pd.read_csv('./data/id_dict_'+ season +'.csv')  #('./data/20'+ season +'/id_dict.csv')\n",
    "\n",
    "    understat_names = [file_.split('.')[0] for file_ in understat_files]\n",
    "    fpl_file_names = [file_.split('.')[0] for file_ in fpl_files]\n",
    "    for name in fpl_file_names:\n",
    "        fpl_player = player_ids[player_ids['FPL_Name'] == name]\n",
    "        fpl_player['FPL_Name'].values\n",
    "\n",
    "        if fpl_player['FPL_Name'].values.size > 0:\n",
    "            fpl_player_name = fpl_player['FPL_Name'].values[0]\n",
    "            understat_player_name = fpl_player['Understat_Name'].values[0]\n",
    "\n",
    "            fpl_player_data = pd.read_csv(joint_fpl_data_path + fpl_player_name+ '.csv')\n",
    "            understat_player_data = pd.read_csv(joint_understat_path + understat_player_name + '.csv')\n",
    "\n",
    "            # Change 'kickoff_time' column name to 'date\n",
    "            fpl_player_data = fpl_player_data.rename(columns={'kickoff_time': 'date'})\n",
    "            # change the formats: From 2021-10-03T13:00:00Z to 2021-10-03\n",
    "            fpl_player_data.date = fpl_player_data.date.apply(lambda x: x.split('T')[0])\n",
    "\n",
    "            # Dates are of the form 2021-10-03T13:00:00Z\n",
    "            fpl_dates_min = fpl_player_data['date'].min()\n",
    "            fpl_dates_max = fpl_player_data['date'].max()\n",
    "\n",
    "\n",
    "            # Filter out player info not in the range of dates we are dealing with\n",
    "            understat_filtered = understat_player_data[(pd.to_datetime(understat_player_data['date']) >= pd.to_datetime(fpl_dates_min))\n",
    "                                                        & (pd.to_datetime(understat_player_data['date']) <= pd.to_datetime(fpl_dates_max) )]\n",
    "\n",
    "            # Marge fpl_player_data with understat_player_data if the dates match\n",
    "            player_data_merged = fpl_player_data.merge(understat_filtered, on=\"date\")\n",
    "\n",
    "            # print(player_data_merged['a_team'])\n",
    "            def set_player_team(row):\n",
    "                player_team = None\n",
    "                if row['was_home']:\n",
    "                    print('home.................')\n",
    "                    player_team = row['h_team']\n",
    "                elif not row['was_home']:\n",
    "                    print('not home.................')\n",
    "                    player_team = row['a_team']\n",
    "                    print('hjjjjjjjjjjjjjjjjjjjj ',pd.Series([player_team]))\n",
    "                return pd.Series([player_team])\n",
    "\n",
    "            with_team = player_data_merged.apply(set_player_team, axis=1)\n",
    "            player_data_merged[['team']] = player_data_merged.apply(set_player_team, axis=1)\n",
    "            print('nnnnnnnnnnnnnnnnnnnnnnn',player_data_merged.shape,  with_team)\n",
    "\n",
    "            # print(player_data_merged['team'])\n",
    "            # if(player_data_merged.shape[0]):\n",
    "            #     merged_dir = './data/joint/'+ season +'/merged/'\n",
    "            #     if not os.path.exists(merged_dir):\n",
    "            #         os.makedirs(merged_dir)\n",
    "\n",
    "            #     player_data_merged.to_csv(merged_dir+ fpl_player_name +'.csv', index_label=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully cleaned 2024-25 fpl data\n",
      "successfully cleaned understat 2024-25 data\n",
      "2024-25 fpl and understat data now in `joint` folder\n",
      "not home.................\n",
      "hjjjjjjjjjjjjjjjjjjjj  0    West Ham\n",
      "dtype: object\n",
      "home.................\n",
      "home.................\n",
      "home.................\n",
      "home.................\n",
      "not home.................\n",
      "hjjjjjjjjjjjjjjjjjjjj  0    West Ham\n",
      "dtype: object\n",
      "not home.................\n",
      "hjjjjjjjjjjjjjjjjjjjj  0    West Ham\n",
      "dtype: object\n",
      "home.................\n",
      "home.................\n",
      "home.................\n",
      "home.................\n",
      "not home.................\n",
      "hjjjjjjjjjjjjjjjjjjjj  0    West Ham\n",
      "dtype: object\n",
      "nnnnnnnnnnnnnnnnnnnnnnn (6, 57)           0\n",
      "0  West Ham\n",
      "1  West Ham\n",
      "2  West Ham\n",
      "3  West Ham\n",
      "4  West Ham\n",
      "5  West Ham\n",
      "home.................\n",
      "home.................\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Columns must be same length as key",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m merge_fpl_understat_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/vaastav/data/2024-25/players/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/vaastav/data/2024-25/understat/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m24-25\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[99], line 54\u001b[0m, in \u001b[0;36mmerge_fpl_understat_data\u001b[1;34m(fpl_player_folder_path, understat_player_folder_path, season)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mSeries([player_team])\n\u001b[0;32m     53\u001b[0m with_team \u001b[38;5;241m=\u001b[39m player_data_merged\u001b[38;5;241m.\u001b[39mapply(set_player_team, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 54\u001b[0m player_data_merged[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteam\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m player_data_merged\u001b[38;5;241m.\u001b[39mapply(set_player_team, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnnnnnnnnnnnnnnnnnnnnnnn\u001b[39m\u001b[38;5;124m'\u001b[39m,player_data_merged\u001b[38;5;241m.\u001b[39mshape,  with_team)\n",
      "File \u001b[1;32mc:\\Users\\ilyandho\\anaconda3\\envs\\FPL\\Lib\\site-packages\\pandas\\core\\frame.py:4082\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4080\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_frame(key, value)\n\u001b[0;32m   4081\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (Series, np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mlist\u001b[39m, Index)):\n\u001b[1;32m-> 4082\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array(key, value)\n\u001b[0;32m   4083\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[0;32m   4084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item_frame_value(key, value)\n",
      "File \u001b[1;32mc:\\Users\\ilyandho\\anaconda3\\envs\\FPL\\Lib\\site-packages\\pandas\\core\\frame.py:4124\u001b[0m, in \u001b[0;36mDataFrame._setitem_array\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4120\u001b[0m     \u001b[38;5;66;03m# Note: unlike self.iloc[:, indexer] = value, this will\u001b[39;00m\n\u001b[0;32m   4121\u001b[0m     \u001b[38;5;66;03m#  never try to overwrite values inplace\u001b[39;00m\n\u001b[0;32m   4123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[1;32m-> 4124\u001b[0m         check_key_length(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, key, value)\n\u001b[0;32m   4125\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k1, k2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(key, value\u001b[38;5;241m.\u001b[39mcolumns):\n\u001b[0;32m   4126\u001b[0m             \u001b[38;5;28mself\u001b[39m[k1] \u001b[38;5;241m=\u001b[39m value[k2]\n",
      "File \u001b[1;32mc:\\Users\\ilyandho\\anaconda3\\envs\\FPL\\Lib\\site-packages\\pandas\\core\\indexers\\utils.py:390\u001b[0m, in \u001b[0;36mcheck_key_length\u001b[1;34m(columns, key, value)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m columns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(key):\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns must be same length as key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;66;03m# Missing keys in columns are represented as -1\u001b[39;00m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(key)[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(value\u001b[38;5;241m.\u001b[39mcolumns):\n",
      "\u001b[1;31mValueError\u001b[0m: Columns must be same length as key"
     ]
    }
   ],
   "source": [
    "merge_fpl_understat_data(\"./data/vaastav/data/2024-25/players/\", \"./data/vaastav/data/2024-25/understat/\", \"24-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_difficulty(season, gwk=None):\n",
    "    print('====> Starting to add difficulty features to 20'+season)\n",
    "    merged = './data/joint/' + season + '/merged/'\n",
    "\n",
    "    player_names = next(os.walk((merged), (None, None, [])))[2]\n",
    "    fixtures = pd.read_csv('./data/20' + season + '/fixtures.csv')\n",
    "\n",
    "    # Loop over each player file in player_names\n",
    "    for name in player_names:\n",
    "        # Load player data\n",
    "        player = pd.read_csv('./data/joint/' + season + '/merged/' + name)\n",
    "\n",
    "        # Function to get the difficulty and was_home columns based on the fixture\n",
    "        def get_fixture_info(row):\n",
    "            # Filter the relevant fixture\n",
    "            fixture = fixtures[fixtures['id'] == row['fixture']]\n",
    "            if not fixture.empty:\n",
    "                fixture = fixture.iloc[0]  # Get the first (and only) match\n",
    "\n",
    "                # Get the team difficulties\n",
    "                team_h_difficulty = fixture['team_h_difficulty']\n",
    "                team_a_difficulty = fixture['team_a_difficulty']\n",
    "                event = fixture['event']\n",
    "\n",
    "                return pd.Series([team_h_difficulty, team_a_difficulty, event])\n",
    "            else:\n",
    "                # Return NaN if no matching fixture found\n",
    "                return pd.Series([None, None, None])\n",
    "\n",
    "        # Apply the function to each row of player\n",
    "        player[['team_h_difficulty', 'team_a_difficulty', 'event']] = player.apply(get_fixture_info, axis=1)\n",
    "\n",
    "\n",
    "        # Save the updated DataFrame with the new columns\n",
    "        new_col_dir = './data/joint/' + season + '/merged_extras/'\n",
    "        if not os.path.exists(new_col_dir):\n",
    "            os.makedirs(new_col_dir)\n",
    "        player.to_csv(new_col_dir + name, index=False)\n",
    "\n",
    "    # print(player[player['round']== 10])\n",
    "    print('****> successfully added difficulty features to 20'+season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_xP(season, gwk=None):\n",
    "    print('================> starting season 20'+season)\n",
    "    players_paths = next(os.walk('./data/joint/'+ season +'/merged_extras', [None], [None],[]))[2]\n",
    "    # players_paths\n",
    "    for path in players_paths:\n",
    "        player = pd.read_csv('./data/joint/'+ season +'/merged_extras/'+ path)\n",
    "        merged = pd.read_csv('./data/20'+ season +'/gws/merged_gw.csv')\n",
    "\n",
    "        player = player.drop(['position'], axis=1)\n",
    "        merged_player = pd.merge(player, merged[['element', 'fixture', 'xP','position']], on=['element', 'fixture'], how='left')\n",
    "\n",
    "\n",
    "        if gwk:\n",
    "            merged_player = merged_player.reindex(merged_player.index.tolist()  + list([merged_player.index[-1]+1]))\n",
    "            merged_player.iloc[-1, merged_player.columns.get_loc('event')] = gwk\n",
    "\n",
    "\n",
    "        # Save the updated DataFrame with the new columns\n",
    "        new_col_dir = './data/joint/'+ season +'/merged_extras_xP/'\n",
    "        if not os.path.exists(new_col_dir):\n",
    "            os.makedirs(new_col_dir)\n",
    "        merged_player.to_csv(new_col_dir + path, index=False)\n",
    "\n",
    "    print('<<<<================ starting season 20'+season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_avgs_3(season, gwk=None):\n",
    "    print('================> starting season 20'+season)\n",
    "    players_paths = next(os.walk('./data/joint/'+ season +'/merged_extras_xP', [None], [None],[]))[2]\n",
    "    for path in players_paths:\n",
    "        player = pd.read_csv('./data/joint/'+ season +'/merged_extras_xP/'+ path,sep=',', skipinitialspace=True)\n",
    "\n",
    "        prev = [\n",
    "                [0 for i in range(34)],\n",
    "                [0 for i in range(34)],\n",
    "                [0 for i in range(34)],\n",
    "                [0 for i in range(34)]\n",
    "            ]\n",
    "        gwks = [1]\n",
    "\n",
    "        features = ['clean_sheets', 'expected_assists', 'expected_goal_involvements', 'expected_goals', 'expected_goals_conceded', 'goals_conceded', 'goals_scored', 'ict_index',\n",
    "                    'influence', 'creativity', 'threat', 'minutes', 'own_goals', 'penalties_missed', 'penalties_saved', 'red_cards', 'yellow_cards', 'saves', 'starts',\n",
    "                    'team_a_score', 'team_h_score', 'total_points', 'goals', 'shots', 'xG', 'xA', 'assists_y', 'key_passes', 'npg', 'npxG', 'xGChain',  'xGBuildup',  'xP', 'selected'\n",
    "                    ]\n",
    "\n",
    "        def rolling(row):\n",
    "            row_items = [row.get(col, 0) for col in features]\n",
    "\n",
    "            if row['event'] - gwks[0] == 0:\n",
    "                del prev[3]\n",
    "                prev.append(row_items)\n",
    "\n",
    "            elif row['event'] - gwks[0] == 1:\n",
    "                del prev[0]\n",
    "                prev.append(row_items)\n",
    "                gwks[0] = row['event']\n",
    "\n",
    "            elif row['event'] - gwks[0] == 2:\n",
    "                del prev[0]\n",
    "                del prev[0]\n",
    "                prev.append([0 for i in range(34)])\n",
    "                prev.append(row_items)\n",
    "                gwks[0] = row['event']\n",
    "\n",
    "            else:\n",
    "                del prev[0]\n",
    "                del prev[0]\n",
    "                del prev[0]\n",
    "                del prev[0]\n",
    "                prev.append([0 for i in range(34)])\n",
    "                prev.append([0 for i in range(34)])\n",
    "                prev.append([0 for i in range(34)])\n",
    "                prev.append(row_items)\n",
    "                gwks[0] = row['event']\n",
    "\n",
    "            # print(row['event'], len(pd.Series([round((x+y+z)/3 ,2) for x,y,z in zip(prev[0], prev[1], prev[2])])),  prev[0], prev[1], prev[2], prev[3]) #\n",
    "            return pd.Series([round((x+y+z) ,2) for x,y,z in zip(prev[0], prev[1], prev[2])])\n",
    "        player[[f\"{col}_3\" for col in features]] = player.apply(rolling, axis=1)\n",
    "\n",
    "        # if curr_sns:\n",
    "        #     tar_wk = pd.Series([round((x+y+z) ,2) for x,y,z in zip(prev[1], prev[2], prev[3])])\n",
    "        #     print( tar_wk )\n",
    "\n",
    "        # Save the updated DataFrame with the new columns\n",
    "        new_col_dir = f'./data/joint/{season}/merged_extras_rolled_3/'\n",
    "        if not os.path.exists(new_col_dir):\n",
    "            os.makedirs(new_col_dir)\n",
    "        player.to_csv(new_col_dir + path, index=False)\n",
    "\n",
    "    print('<<<<================ starting season 20'+season)\n",
    "\n",
    "def add_rolling_avgs_5(season):\n",
    "    print('================> starting season 20'+season)\n",
    "    players_paths = next(os.walk('./data/joint/'+ season +'/merged_extras_rolled_3', [None], [None],[]))[2]\n",
    "    for path in players_paths:\n",
    "        player = pd.read_csv('./data/joint/'+ season +'/merged_extras_rolled_3/'+ path,sep=',', skipinitialspace=True)\n",
    "\n",
    "        prev = [\n",
    "                [0 for i in range(34)],\n",
    "                [0 for i in range(34)],\n",
    "                [0 for i in range(34)],\n",
    "                [0 for i in range(34)],\n",
    "                [0 for i in range(34)],\n",
    "                [0 for i in range(34)]\n",
    "            ]\n",
    "        gwks = [1]\n",
    "\n",
    "        features = ['clean_sheets', 'expected_assists', 'expected_goal_involvements', 'expected_goals', 'expected_goals_conceded', 'goals_conceded', 'goals_scored', 'ict_index',\n",
    "                    'influence', 'creativity', 'threat', 'minutes', 'own_goals', 'penalties_missed', 'penalties_saved', 'red_cards', 'yellow_cards', 'saves', 'starts',\n",
    "                    'team_a_score', 'team_h_score', 'total_points', 'goals', 'shots', 'xG', 'xA', 'assists_y', 'key_passes', 'npg', 'npxG', 'xGChain',  'xGBuildup',  'xP', 'selected'\n",
    "                    ]\n",
    "\n",
    "        def rolling(row):\n",
    "            row_items = [row.get(col, 0) for col in features]\n",
    "\n",
    "            if row['event'] - gwks[0]== 0:\n",
    "                del prev[5]\n",
    "                prev.append(row_items)\n",
    "\n",
    "            elif row['event'] - gwks[0] == 1:\n",
    "                del prev[0]\n",
    "                prev.append(row_items)\n",
    "                gwks[0] = row['event']\n",
    "\n",
    "            elif row['event'] - gwks[0] == 2:\n",
    "                del prev[0]\n",
    "                del prev[0]\n",
    "                prev.append([0 for i in range(34)])\n",
    "                prev.append(row_items)\n",
    "                gwks[0] = row['event']\n",
    "\n",
    "            elif row['event'] - gwks[0] == 3:\n",
    "                del prev[0]\n",
    "                del prev[0]\n",
    "                del prev[0]\n",
    "                prev.append([0 for i in range(34)])\n",
    "                prev.append([0 for i in range(34)])\n",
    "                prev.append(row_items)\n",
    "                gwks[0] = row['event']\n",
    "\n",
    "            elif row['event'] - gwks[0] == 4:\n",
    "                del prev[0]\n",
    "                del prev[0]\n",
    "                del prev[0]\n",
    "                del prev[0]\n",
    "                prev.append([0 for i in range(34)])\n",
    "                prev.append([0 for i in range(34)])\n",
    "                prev.append([0 for i in range(34)])\n",
    "                prev.append(row_items)\n",
    "                gwks[0] = row['event']\n",
    "            else:\n",
    "                del prev[0]\n",
    "                del prev[0]\n",
    "                del prev[0]\n",
    "                del prev[0]\n",
    "                del prev[0]\n",
    "                del prev[0]\n",
    "                prev.append([0 for i in range(34)])\n",
    "                prev.append([0 for i in range(34)])\n",
    "                prev.append([0 for i in range(34)])\n",
    "                prev.append([0 for i in range(34)])\n",
    "                prev.append([0 for i in range(34)])\n",
    "                prev.append(row_items)\n",
    "                gwks[0] = row['event']\n",
    "\n",
    "            return pd.Series([round((v+w+x+y+z) ,2) for v, w, x,y,z in zip(prev[0], prev[1], prev[2], prev[3], prev[4])])\n",
    "        player[[f\"{col}_5\" for col in features]] = player.apply(rolling, axis=1)\n",
    "        print(player)\n",
    "\n",
    "        # Save the updated DataFrame with the new columns\n",
    "        new_col_dir = f'./data/joint/{season}/merged_extras_rolled_5/'\n",
    "        if not os.path.exists(new_col_dir):\n",
    "            os.makedirs(new_col_dir)\n",
    "        player.to_csv(new_col_dir + path, index=False)\n",
    "\n",
    "def add_rolling_avgs(season, gwk=None):\n",
    "    add_rolling_avgs_3(season, gwk)\n",
    "    add_rolling_avgs_5(season)\n",
    "\n",
    "    print('<<<<================ Done rolling season 20'+season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odds(sns, nxt_gw=0):\n",
    "    print('================> starting sns 20'+sns)\n",
    "    odds_nxt = pd.read_csv(f'./data/odds/odds_{nxt_gw}.csv')\n",
    "    # print(odds_nxt)\n",
    "    # print(nxt_gw)\n",
    "    # Load the data data once to avoid redundant file reads\n",
    "    data = pd.read_csv('./data/odds/E0 '+ sns +'.csv')\n",
    "    data = data.rename(columns={'HomeTeam': 'h_team', 'AwayTeam': 'a_team'})\n",
    "\n",
    "    players_paths = next(os.walk('./data/joint/'+ sns +'/merged_extras_rolled_5', [None], [None],[]))[2]\n",
    "    for path in players_paths:\n",
    "        rolled = pd.read_csv('./data/joint/'+ sns +'/merged_extras_rolled_5/'+ path,sep=',', skipinitialspace=True)\n",
    "\n",
    "        def add_odds(row, data):\n",
    "            # Filter the data DataFrame for the matching teams\n",
    "            match = data[(data['h_team'] == row['h_team']) & (data['a_team'] == row['a_team'])]\n",
    "            # Check if a match is found\n",
    "\n",
    "            if not match.empty:\n",
    "                # Extract the relevant data values\n",
    "                # Convert the data to probabilities\n",
    "                odds_ = match.iloc[0]\n",
    "\n",
    "                # print('===========================++++++ odds ', odds_['WHH'], odds_['WHD'], odds_['WHA'])\n",
    "                WHH = round(1/odds_['WHH'], 5)\n",
    "                WHD = round(1/odds_['WHD'], 5)\n",
    "                WHA = round(1/odds_['WHA'], 5)\n",
    "\n",
    "                # Normalize the probabilities (to make the probabilities sum to 100%)\n",
    "                WH_sum = WHH + WHD + WHA\n",
    "                WHH_ = round(WHH/WH_sum, 3)\n",
    "                WHD_ = round(WHD/WH_sum, 3)\n",
    "                WHA_ = round(WHA/WH_sum, 3)\n",
    "\n",
    "                # print('===========================++++++ odds ', odds_['WHH'], odds_['WHD'], odds_['WHA'], WHH_, WHD_, WHA_)\n",
    "                pts_bps = row['total_points'] - row['bonus']\n",
    "                return pd.Series([pts_bps, WHH_, WHD_, WHA_])\n",
    "            else:\n",
    "                # print(row['h_team'], row['a_team'], '==========================>>>>', data['h_team'], data['a_team'])\n",
    "                # Return NaN for rows with no match\n",
    "                return pd.Series([None,None, None, None])\n",
    "\n",
    "        # Apply the function to the 'rolled' DataFrame\n",
    "        rolled[['pts_bps','whh', 'whd', 'wha']] = rolled.apply(add_odds, axis=1, data=data)\n",
    "\n",
    "        if nxt_gw:\n",
    "            opp_team = rolled['opponent_team']\n",
    "            was_home = rolled['was_home']\n",
    "\n",
    "\n",
    "            # print(rolled['h_team'], rolled['a_team'])\n",
    "            # rolled.iloc[-1, rolled.columns.get_loc('whh')] = 1\n",
    "            # rolled.iloc[-1, rolled.columns.get_loc('whd')] = 2\n",
    "            # rolled.iloc[-1, rolled.columns.get_loc('wha')] = 3\n",
    "            # Merge and update probabilities\n",
    "            # if rolled['']\n",
    "\n",
    "            print(rolled['a_team'])\n",
    "\n",
    "\n",
    "\n",
    "        # # Save the updated DataFrame with the new columns\n",
    "        # new_col_dir = './data/joint/'+ sns +'/merged_extras_odds/'\n",
    "        # if not os.path.exists(new_col_dir):\n",
    "        #     os.makedirs(new_col_dir)\n",
    "        # rolled.to_csv(new_col_dir + path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('./data/joint/24-25/merged_extras_rolled_5/Aaron Cresswell.csv')[[ 'a_team', 'h_team']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odds('24-25', 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('./data/joint/24-25/merged_extras_odds/Bruno Borges Fernandes.csv') #[['h_team', 'a_team', 'opponent_team']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(season):\n",
    "    paths = next(os.walk('./data/joint/'+ season +'/merged_extras_odds', [None], [None],[]))[2]\n",
    "    files_list = [pd.read_csv('./data/joint/'+ season +'/merged_extras_odds/' + path)  for  path in paths ]\n",
    "    merged_files = pd.concat(files_list)\n",
    "\n",
    "    # Save the new DataFrame\n",
    "    new_col_dir = './data/joint/'+ season +'/'\n",
    "\n",
    "    merged_files.to_csv(new_col_dir  +'merged_player_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
