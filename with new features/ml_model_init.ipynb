{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor, ColumnTransformer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from xgboost import XGBRegressor as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.pipeline import Pipeline  # Use the imblearn pipeline\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the linear model\n",
    "def Linear_regression(features_train, features_test, target_train, target_test):\n",
    "    # Before using our data, we need to do feature scaling and we opt for the 'standardization' method of scaling.\n",
    "    # The 'standardization' is avaliable thorugh the StandardScaler() method\n",
    "    # Transformers help in batching tasks in a pipepline. In this case, the data is scaled and then a linear regression model is fitted on the scaled data.\n",
    "    # We use a transformer that takes the regression model and the transformation method\n",
    "    # The TransformedTargetRegressor does the transformation and when we do the prediction, it automatically does the inverse transformation (scaling) and returns the values\n",
    "    bool_cols = features_train.drop(columns=['was_home']).columns.tolist()\n",
    "    # categorical_cols = ['was_home']\n",
    "    bool_cols = features_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_cols = features_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('one_hot_encoder', OneHotEncoder())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, bool_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "        ])\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    model = TransformedTargetRegressor(regressor=pipeline, transformer=StandardScaler())\n",
    "\n",
    "    # TransformedTargetRegressor(\n",
    "    #     LinearRegression(), transformer=StandardScaler())\n",
    "\n",
    "    # fit the transofrmer on the train data\n",
    "    model.fit(features_train, target_train)\n",
    "\n",
    "    # With the model fitted, we can predict the total_points given the feature_train and feature_test set\n",
    "    pred_train = model.predict(features_train)\n",
    "    pred_test = model.predict(features_test)\n",
    "\n",
    "    # Evaluate the performance of the model on both sets using the mean absolute error\n",
    "    train_MAE = mean_absolute_error(target_train, pred_train)\n",
    "    test_MAE = mean_absolute_error(target_test, pred_test)\n",
    "\n",
    "    # Evaluate the performance of the model on both sets using the mean square error\n",
    "    train_MSE = mean_squared_error(target_train, pred_train)\n",
    "    test_MSE = mean_squared_error(target_test, pred_test)\n",
    "\n",
    "    # Evaluate the performance of the model on both sets using the root mean square error\n",
    "    train_RMSE = mean_squared_error(target_train, pred_train, squared=False)\n",
    "    test_RMSE = mean_squared_error(target_test, pred_test, squared=False)\n",
    "\n",
    "    # Get the score of the model or the coeeficient of determination i.e how much of the target value can be explained by the model.\n",
    "    # In this case, 0.6 implies that 60% of the variations in the target value can be explained by the model and 40% is not explainable\n",
    "    R2_train = model.score(features_train, target_train)\n",
    "    R2_test = model.score(features_test, target_test)\n",
    "\n",
    "    # If the test error significantly differs from the train error, then there is either overfitting or underfitting\n",
    "    # RMSE, just like the squared loss function that it derives from, effectively penalizes larger errors more severely.\n",
    "    print('Training set RMSE: {}'.format(train_RMSE))\n",
    "    print('Test set RMSE: {}'.format(test_RMSE))\n",
    "\n",
    "    print('Training set R2: {}'.format(R2_train))\n",
    "    print('Test set R2: {}'.format(R2_test))\n",
    "\n",
    "    # Carry out cross validation of the model.\n",
    "    # The evaluation method is the root mean square error\n",
    "    # The method expects a utility function (greater is better) and so the scoring function is the opposite of the the RMSE. Hence the -ve\n",
    "    tree_rmses = -cross_val_score(model, features_train, target_train,\n",
    "                                  scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "\n",
    "    return {'train_MAE': train_MAE, 'test_MAE': test_MAE, 'train_MSE': train_MSE, 'test_MSE': test_MSE, 'train_RMSE': train_RMSE, 'test_RMSE': test_RMSE, 'cv_rmse': tree_rmses.mean(), 'R2_train': R2_train, 'R2_test': R2_test}\n",
    "\n",
    "\n",
    "# Decision Tree Model\n",
    "def DecisionTreeRegression(features_train, features_test, target_train, target_test):\n",
    "    # The DecisionTreeRegressor is passed as the model to the TransformedTreeRegressor together with the StandardScaler\n",
    "    bool_cols = features_train.drop(columns=['was_home']).columns.tolist()\n",
    "    # categorical_cols = ['was_home']\n",
    "    bool_cols = features_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_cols = features_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('one_hot_encoder', OneHotEncoder())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, bool_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "        ])\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', DecisionTreeRegressor())\n",
    "    ])\n",
    "\n",
    "    model = TransformedTargetRegressor(regressor=pipeline, transformer=StandardScaler())\n",
    "    # TransformedTargetRegressor(\n",
    "    #     DecisionTreeRegressor(), transformer=StandardScaler())\n",
    "    model.fit(features_train, target_train)\n",
    "\n",
    "    pred_train = model.predict(features_train)\n",
    "    pred_test = model.predict(features_test)\n",
    "\n",
    "    train_MAE = mean_absolute_error(target_train, pred_train)\n",
    "    test_MAE = mean_absolute_error(target_test, pred_test)\n",
    "\n",
    "    train_MSE = mean_squared_error(target_train, pred_train)\n",
    "    test_MSE = mean_squared_error(target_test, pred_test)\n",
    "\n",
    "    train_RMSE = mean_squared_error(target_train, pred_train, squared=False)\n",
    "    test_RMSE = mean_squared_error(target_test, pred_test, squared=False)\n",
    "\n",
    "    R2_train = model.score(features_train, target_train)\n",
    "    R2_test = model.score(features_test, target_test)\n",
    "\n",
    "    tree_rmses = -cross_val_score(model, features_train, target_train,\n",
    "                                  scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "\n",
    "    return {'train_MAE': train_MAE, 'test_MAE': test_MAE, 'train_MSE': train_MSE, 'test_MSE': test_MSE,\n",
    "            'train_RMSE': train_RMSE, 'test_RMSE': test_RMSE, 'cv_rmse': tree_rmses.mean(), 'R2_train': R2_train, 'R2_test': R2_test}\n",
    "\n",
    "\n",
    "# RandomForestRegressor\n",
    "def RandomForestRegression(features_train, features_test, target_train, target_test, hyperparameters):\n",
    "    # RandomForestRegressor is an ensemble method\n",
    "    # The TransformedTargetRegressor is passed the RandomForestRegressor model\n",
    "    # The RandomForestRegressor is passed some hyper-parameters such as;\n",
    "    # n_esimtaors: number of trees in the forest,\n",
    "    # max_depth: the maximum depth of the tree,\n",
    "    # criterion: the function to measure the quality of the split\n",
    "\n",
    "    bool_cols = features_train.drop(columns=['was_home']).columns.tolist()\n",
    "    # categorical_cols = ['was_home']\n",
    "    bool_cols = features_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_cols = features_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('one_hot_encoder', OneHotEncoder())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, bool_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "        ])\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', RandomForestRegressor(\n",
    "                        n_estimators=hyperparameters['n_estimators'],\n",
    "                        max_depth=hyperparameters['max_depth'],\n",
    "                        criterion=hyperparameters['criterion'], random_state=18\n",
    "                        ),)\n",
    "    ])\n",
    "\n",
    "    model = TransformedTargetRegressor(regressor=pipeline, transformer=StandardScaler())\n",
    "\n",
    "    # model = TransformedTargetRegressor( transformer=StandardScaler())\n",
    "    model.fit(features_train, target_train)\n",
    "\n",
    "    pred_train = model.predict(features_train)\n",
    "    pred_test = model.predict(features_test)\n",
    "\n",
    "    train_MAE = mean_absolute_error(target_train, pred_train)\n",
    "    test_MAE = mean_absolute_error(target_test, pred_test)\n",
    "\n",
    "    train_MSE = mean_squared_error(target_train, pred_train)\n",
    "    test_MSE = mean_squared_error(target_test, pred_test)\n",
    "\n",
    "    train_RMSE = mean_squared_error(target_train, pred_train, squared=False)\n",
    "    test_RMSE = mean_squared_error(target_test, pred_test, squared=False)\n",
    "\n",
    "    R2_train = model.score(features_train, target_train)\n",
    "    R2_test = model.score(features_test, target_test)\n",
    "\n",
    "    tree_rmses = -cross_val_score(model, features_train, target_train,\n",
    "                                  scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "\n",
    "    return {'train_MAE': train_MAE, 'test_MAE': test_MAE, 'train_MSE': train_MSE, 'test_MSE': test_MSE,\n",
    "            'train_RMSE': train_RMSE, 'test_RMSE': test_RMSE, 'cv_rmse': tree_rmses.mean(), 'R2_train': R2_train, 'R2_test': R2_test}\n",
    "\n",
    "\n",
    "def XGBoostRegression(features_train, features_test, target_train, target_test, hyperparameters):\n",
    "\n",
    "    bool_cols = features_train.drop(columns=['was_home']).columns.tolist()\n",
    "    # categorical_cols = ['was_home']\n",
    "    bool_cols = features_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_cols = features_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('one_hot_encoder', OneHotEncoder())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, bool_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "        ])\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', xgb(learning_rate=hyperparameters[\"learning_rate\"],\n",
    "                    n_estimators=hyperparameters[\"n_estimators\"],\n",
    "                    max_depth=hyperparameters[\"max_depth\"],\n",
    "                    eval_metric='rmsle'),)\n",
    "    ])\n",
    "\n",
    "    model = TransformedTargetRegressor(regressor=pipeline, transformer=StandardScaler())\n",
    "\n",
    "    model.fit(features_train, target_train)\n",
    "\n",
    "    # =========================================================================\n",
    "    # To use early_stopping_rounds:\n",
    "    # \"Validation metric needs to improve at least once in every\n",
    "    # early_stopping_rounds round(s) to continue training.\"\n",
    "    # =========================================================================\n",
    "    # first perform a test/train split\n",
    "    # from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # X_train,X_test,y_train,y_test = train_test_split(X_train,y_train, test_size = 0.2)\n",
    "    # model.fit(X_train, y_train, early_stopping_rounds=6, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "    # =========================================================================\n",
    "    # use the model to predict the prices for the test data\n",
    "    # =========================================================================\n",
    "    # predictions = model.predict(goalkeepers_splits['feature_test'])\n",
    "\n",
    "    pred_train = model.predict(features_train)\n",
    "    pred_test = model.predict(features_test)\n",
    "\n",
    "    train_MAE = mean_absolute_error(target_train, pred_train)\n",
    "    test_MAE = mean_absolute_error(target_test, pred_test)\n",
    "\n",
    "    train_MSE = mean_squared_error(target_train, pred_train)\n",
    "    test_MSE = mean_squared_error(target_test, pred_test)\n",
    "\n",
    "    train_RMSE = mean_squared_error(target_train, pred_train, squared=False)\n",
    "    test_RMSE = mean_squared_error(target_test, pred_test, squared=False)\n",
    "\n",
    "    R2_train = model.score(features_train, target_train)\n",
    "    R2_test = model.score(features_test, target_test)\n",
    "\n",
    "    tree_rmses = -cross_val_score(model, features_train, target_train,\n",
    "                                  scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "\n",
    "    return {'train_MAE': train_MAE, 'test_MAE': test_MAE, 'train_MSE': train_MSE, 'test_MSE': test_MSE,\n",
    "            'train_RMSE': train_RMSE, 'test_RMSE': test_RMSE, 'cv_rmse': tree_rmses.mean(), 'R2_train': R2_train, 'R2_test': R2_test}\n",
    "\n",
    "def Logistic_regression(features_train, features_test, target_train, target_test):\n",
    "    encoder = LabelEncoder()\n",
    "    cs_train_ = encoder.fit_transform(target_train)\n",
    "    cs_test_ = encoder.transform(target_test)\n",
    "\n",
    "    # bool_cols = feats_train.drop(columns=['was_home']).columns.tolist()\n",
    "    # categorical_cols = ['was_home']\n",
    "    bool_cols = feats_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_cols = feats_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('one_hot_encoder', OneHotEncoder(sparse=False)),\n",
    "        # ('to_dense', ToDense())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, bool_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "        ])\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('smote', SMOTE(sampling_strategy='auto', random_state=42)),\n",
    "        ('model', LogisticRegression(class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(features_train, cs_train_)\n",
    "\n",
    "    train_score = pipeline.score(features_train, cs_train_)\n",
    "    test_score = pipeline.score(features_test, cs_test_)\n",
    "    # Make predictions on the test set\n",
    "    cs_pred = pipeline.predict(features_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy =  accuracy_score(cs_test_, cs_pred)\n",
    "\n",
    "    conf_mat = confusion_matrix(cs_test_,cs_pred)\n",
    "\n",
    "    class_report = classification_report(cs_test_, cs_pred)\n",
    "\n",
    "    unique, counts = np.unique(cs_test_, return_counts=True)\n",
    "    print(\"Class distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "\n",
    "    return {'train_score': train_score, 'test_score': test_score, 'accuracy': accuracy, 'conf_mat': conf_mat, 'class_report': class_report}\n",
    "\n",
    "\n",
    "def Random_Forest_Classifier(features_train, features_test, target_train, target_test):\n",
    "    encoder = LabelEncoder()\n",
    "    cs_train_ = encoder.fit_transform(target_train)\n",
    "    cs_test_ = encoder.transform(target_test)\n",
    "\n",
    "    # bool_cols = feats_train.drop(columns=['was_home']).columns.tolist()\n",
    "    # categorical_cols = ['was_home']\n",
    "    bool_cols = feats_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_cols = feats_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('one_hot_encoder', OneHotEncoder(sparse=False)),\n",
    "        # ('to_dense', ToDense())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, bool_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "        ])\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('smote', BorderlineSMOTE(sampling_strategy='auto', random_state=42)),  # Apply SMOTE to the data\n",
    "        ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42))  # Random Forest Classifier\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(features_train, cs_train_)\n",
    "\n",
    "    train_score = pipeline.score(features_train, cs_train_)\n",
    "    test_score = pipeline.score(features_test, cs_test_)\n",
    "    # Make predictions on the test set\n",
    "    cs_pred = pipeline.predict(features_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy =  accuracy_score(cs_test_, cs_pred)\n",
    "\n",
    "    conf_mat = confusion_matrix(cs_test_,cs_pred)\n",
    "\n",
    "    class_report = classification_report(cs_test_, cs_pred)\n",
    "\n",
    "    unique, counts = np.unique(cs_test_, return_counts=True)\n",
    "    print(\"Class distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "\n",
    "    return {'train_score': train_score, 'test_score': test_score, 'accuracy': accuracy, 'conf_mat': conf_mat, 'class_report': class_report}\n",
    "\n",
    "\n",
    "def GridSearchParams(features_train, target_train):\n",
    "    # Instatiate the model\n",
    "    model = RandomForestRegressor()\n",
    "\n",
    "    param_grid = {'n_estimators': [8, 10, 12, 14, 16, 18, 20]}\n",
    "\n",
    "    # Define the possible values of the hyperparameter\n",
    "    grid = {\n",
    "        'n_estimators': [8, 10, 12, 14, 16, 18, 20, 200, 300, 400, 500],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'max_depth': [4, 5, 6, 7, 8],\n",
    "        'criterion': ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'],\n",
    "        'random_state': [18]\n",
    "    }\n",
    "\n",
    "    # Deine the model with cv=3 for a 3-fold cross validation\n",
    "    # GridSearchCV has the best_estimator_ parameter that returns the  estimator\n",
    "    # which gave highest score (or smallest loss if specified)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        model, grid, cv=3, scoring='neg_root_mean_squared_error')\n",
    "    grid_search.fit(features_train, target_train)\n",
    "\n",
    "    # Get the best param combination\n",
    "    print(grid_search.best_estimator_)\n",
    "\n",
    "    return {'train_RMSE': train_RMSE, 'test_RMSE': test_RMSE, 'R2_train': R2_train, 'R2_test': R2_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a function that splits and returns features_train, features_test, target_train, target_test\n",
    "\n",
    "def split_data(data):\n",
    "    # Store the 'total_points' target in the 'player_target' variable\n",
    "    # and the rest in the player_features variable\n",
    "    player_target = data['pts_bps']\n",
    "    player_features = data.drop(\"pts_bps\", axis=1)\n",
    "\n",
    "    # The train_test_split function splits the set into train and test sets while maintain the same data distribution over both sets.\n",
    "    # It takes the feature and target sets and reutrns the respective train and test sets\n",
    "    features_train, features_test, target_train, target_test = train_test_split(\n",
    "        player_features, player_target, test_size=0.2)\n",
    "\n",
    "    return {'feature_train': features_train, 'features_test': features_test, 'target_train': target_train, 'target_test': target_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_22_23 = pd.read_csv('./data/joint/22-23/merged_player_data.csv').dropna()\n",
    "data_23_24 = pd.read_csv('./data/joint/23-24/merged_player_data.csv').dropna()\n",
    "data_24_25 = pd.read_csv('./data/joint/24-25/merged_player_data.csv').dropna()\n",
    "data_tar = data_24_25[data_24_25['event']==10]\n",
    "data_24_25 = data_24_25[data_24_25['event'] != 10]\n",
    "\n",
    "data = pd.concat([data_22_23, data_23_24, data_24_25])\n",
    "predicted = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    Total Points – Bonus Points (tp-bp), Minutes, Yellow Cards, Red Cards, Expected Goals (xG), Expected Assists (xA), Non-penalty Expected Goals (npxG),\n",
    "    Shots, Expected Goals Against, _Expected_goal_involvements_,  clean_sheets, ict_index, opponent_team, Expected Goals Buildup (xG Buildup), threat, value,\n",
    "    Key Passes,\n",
    "\n",
    "\n",
    "    _Games_,  Expected Goals Chain (xG Chain),  _Non-penalty Expected Goal Difference (npxGD)_, _Non-penalty Expected Goals Against (npxGA)_, Expected Points (xPts)\n",
    "```\n",
    "\n",
    "```js\n",
    "    Total Points – Bonus Points (tp-bp)\tfor, mid, def, gk\n",
    "    Minutes\tfor, mid, def, gk\n",
    "    Yellow Cards\tfor, mid, def, gk\n",
    "    Red Cards\tfor, mid, def, gk\n",
    "    Expected Goals (xG)\tfor, mid, def, gk\n",
    "    Expected Assists (xA)\tfor, mid, def, gk\n",
    "    Non-penalty Expected Goals (npxG)\tfor, mid, def\n",
    "    Games\tfor, mid, def, gk\n",
    "    Shots\tfor, mid, def\n",
    "    Key Passes\tfor, mid, def, gk\n",
    "    Expected Goals Chain (xG Chain)\tfor, mid, def\n",
    "    Expected Goals Buildup (xG Buildup)\tfor, mid, def\n",
    "    Non-penalty Expected Goal Difference (npxGD)\tdef, gk\n",
    "    Expected Goals Against\tdef, gk\n",
    "    Non-penalty Expected Goals Against (npxGA)\tdef, gk\n",
    "    Expected Points (xPts)\tdef, gk\n",
    "    expected_goal_involvements\tfor, mid, def, gk\n",
    "    clean_sheets\tmid, def, gk\n",
    "    ict_index\tfor, mid, def, gk\n",
    "    opponent_team\tfor, mid, def, gk\n",
    "    threat\tfor, mid, def, gk\n",
    "    value\tfor, mid, def, gk\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cols =[\n",
    "# #        'assists_x', 'bonus', 'goals_conceded', 'goals_scored', 'ict_index', 'red_cards', 'round', 'selected', 'threat', 'total_points', 'transfers_in', 'transfers_out',\n",
    "# #        'value', 'was_home', 'xG','season', 'npg', 'npxG', 'xGChain', 'xGBuildup', 'team_h_difficulty', 'team_a_difficulty', 'season', 'event', 'position', 'clean_sheets_3',\n",
    "# #        'expected_assists_3', 'expected_goal_involvements_3', 'expected_goals_3', 'expected_goals_conceded_3', 'goals_conceded_3', 'goals_scored_3', 'influence_3', 'minutes_3',\n",
    "# #        'penalties_missed_3', 'penalties_saved_3', 'red_cards_3', 'saves_3', 'starts_3', 'team_a_score_3', 'team_h_score_3', 'total_points_3', 'yellow_cards_3', 'goals_3',\n",
    "# #        'shots_3', 'xA_3', 'key_passes_3', 'npg_3', 'npxG_3', 'xGChain_3', 'xGBuildup_3', 'xP_3', 'pts_bps', 'WHH', 'WHD', 'WHA'\n",
    "# #  ]\n",
    "\n",
    "\n",
    "# cols =[\n",
    "#        'red_cards', 'selected', 'threat', 'transfers_in', 'transfers_out',\n",
    "#        'value', 'was_home', 'xG','season', 'npg', 'npxG', 'xGChain', 'xGBuildup', 'team_h_difficulty', 'team_a_difficulty', 'season', 'event', 'position', 'clean_sheets_3',\n",
    "#        'expected_assists_3', 'expected_goal_involvements_3', 'expected_goals_3', 'expected_goals_conceded_3', 'goals_conceded_3', 'goals_scored_3', 'influence_3', 'minutes_3',\n",
    "#        'penalties_missed_3', 'penalties_saved_3', 'red_cards_3', 'saves_3', 'starts_3', 'team_a_score_3', 'team_h_score_3', 'total_points_3', 'yellow_cards_3', 'goals_3',\n",
    "#        'shots_3', 'xA_3', 'key_passes_3', 'npg_3', 'npxG_3', 'xGChain_3', 'xGBuildup_3', 'xP_3', 'pts_bps', 'WHH', 'WHD', 'WHA'\n",
    "#  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = [\n",
    "#     'total_points', 'bonus', 'minutes', 'yellow_cards', 'red_cards', 'expected_goals', 'expected_assists', 'npxG', 'shots', 'expected_goal_involvements', 'expected_goals_conceded', 'clean_sheets', 'ict_index',\n",
    "#     'xGBuildup', 'threat', 'value', 'key_passes', 'xGChain', 'xP', 'team_h_difficulty', 'team_a_difficulty', 'was_home', 'position']\n",
    "# gk_cols = [\n",
    "#        # 'transfers_in', 'transfers_out', 'value',  'xG','season', 'npg', 'npxG', 'xGChain', 'xGBuildup', 'season', 'event', 'penalties_missed_3', 'penalties_saved_3',\n",
    "#        #  'team_a_score_3', 'team_h_score_3', 'position',  # 'total_points_3',  'goals_3', 'shots_3', 'xA_3',  'npg_3', 'npxG_3',\n",
    "#        'bps', 'influence', 'creativity', 'threat', 'selected_3', 'pts_bps', 'minutes_3', 'yellow_cards_3', 'red_cards_3', 'expected_assists_3', 'expected_goal_involvements_3', 'expected_goals_conceded_3', 'clean_sheets_3',\n",
    "#        'goals_conceded_3',  'ict_index_3', 'influence_3', 'creativity_3', 'threat_3', 'key_passes_3', 'xP_3', 'team_h_difficulty', 'team_a_difficulty', 'xGChain_3', 'xGBuildup_3',\n",
    "#        'saves_3', 'starts_3','was_home', 'WHH', 'WHD', 'WHA'\n",
    "#        ]\n",
    "\n",
    "# def_cols = [\n",
    "#        # 'threat', 'transfers_in', 'transfers_out', 'value', 'season', 'npg', 'npxG', 'xGChain', 'xGBuildup', 'season', 'event', 'penalties_missed_3',\n",
    "#        # 'penalties_saved_3',   'team_a_score_3', 'team_h_score_3', 'total_points_3',  'goals_3', 'npg_3', 'npxG_3','position',\n",
    "\n",
    "#        'selected_3', 'pts_bps', 'minutes_3', 'yellow_cards_3', 'red_cards_3', 'expected_goals_3', 'xG', 'goals_scored_3', 'expected_assists_3', 'xA_3', 'expected_goal_involvements_3',\n",
    "#        'expected_goals_conceded_3', 'clean_sheets_3', 'goals_conceded_3',  'ict_index_3', 'influence_3', 'creativity_3', 'threat_3', 'key_passes_3', 'xP_3', 'team_h_difficulty',\n",
    "#        'team_a_difficulty', 'xGChain_3', 'xGBuildup_3', 'saves_3', 'starts_3', 'shots_3', 'was_home', 'WHH', 'WHD', 'WHA'\n",
    "#        ]\n",
    "\n",
    "# mid_cols = [\n",
    "#        # 'threat', 'transfers_in', 'transfers_out', 'value', 'season', 'npg', 'npxG', 'xGChain', 'xGBuildup', 'season', 'event', 'penalties_missed_3',\n",
    "#        # 'penalties_saved_3',   'team_a_score_3', 'team_h_score_3', 'total_points_3',  'npg_3', 'npxG_3','position',\n",
    "\n",
    "#        'selected_3', 'pts_bps', 'minutes_3', 'yellow_cards_3', 'red_cards_3', 'expected_goals_3', 'xG', 'goals_3', 'goals_scored_3', 'expected_assists_3', 'xA_3',\n",
    "#        'expected_goal_involvements_3', 'expected_goals_conceded_3', 'clean_sheets_3', 'goals_conceded_3',  'ict_index_3', 'influence_3', 'creativity_3', 'threat_3', 'key_passes_3', 'xP_3',\n",
    "#        'team_h_difficulty', 'team_a_difficulty', 'xGChain_3', 'xGBuildup_3',  'starts_3', 'shots_3', 'was_home', 'WHH', 'WHD', 'WHA'\n",
    "#        ]\n",
    "\n",
    "# fwd_cols =[\n",
    "#        #  'threat', 'transfers_in', 'transfers_out', 'value', 'season', 'npg', 'npxG', 'xGChain', 'xGBuildup', 'season', 'event', 'penalties_missed_3',\n",
    "#        # 'penalties_saved_3',   'team_a_score_3', 'team_h_score_3', 'total_points_3',  'npg_3', 'npxG_3', 'expected_goals_conceded_3', 'clean_sheets_3', 'goals_conceded_3','position',\n",
    "\n",
    "#        'selected_3', 'pts_bps', 'minutes_3', 'yellow_cards_3', 'red_cards_3', 'expected_goals_3', 'xG', 'goals_3', 'goals_scored_3', 'expected_assists_3', 'xA_3',\n",
    "#        'expected_goal_involvements_3', 'ict_index_3', 'influence_3', 'creativity_3', 'threat_3', 'key_passes_3', 'xP_3', 'team_h_difficulty', 'team_a_difficulty', 'xGChain_3', 'xGBuildup_3',\n",
    "#        'starts_3', 'shots_3', 'was_home', 'WHH', 'WHD', 'WHA'\n",
    "#        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # player_21_22 = pd.read_csv('./data/joint/21-22/merged_player_data.csv')[columns]\n",
    "# player_22_23 = pd.read_csv('./data/joint/22-23/merged_player_data.csv')\n",
    "# player_23_24 = pd.read_csv('./data/joint/23-24/merged_player_data.csv')\n",
    "# player_24_25 = pd.read_csv('./data/joint/24-25/merged_player_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Filter out players with zero points\n",
    "# player_22_23_no = player_22_23[player_22_23['total_points'] !=0]\n",
    "# player_23_24_no = player_23_24[player_23_24['total_points'] !=0]\n",
    "# player_24_25_no = player_24_25[player_24_25['total_points'] !=0]\n",
    "\n",
    "# player_data_all = pd.concat([player_22_23_no, player_23_24_no, player_24_25_no]).dropna()\n",
    "\n",
    "# # def points_(row):\n",
    "# #     return row['total_points'] - row['bonus']\n",
    "# # player_data['pts_bps'] = player_data.apply(points_, axis=1)\n",
    "# # player_data = player_data.drop(['total_points', 'bonus'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Group by position\n",
    "# gk_player_data = player_data_all[player_data_all['position']=='GK']\n",
    "# gk_player_data = gk_player_data.drop('position', axis=1)\n",
    "# gk_player_data = gk_player_data[gk_cols]\n",
    "\n",
    "# def_player_data = player_data_all[player_data_all['position']=='DEF']\n",
    "# def_player_data = def_player_data.drop('position', axis=1)\n",
    "# def_player_data = def_player_data[def_cols]\n",
    "\n",
    "# mid_player_data = player_data_all[player_data_all['position']=='MID']\n",
    "# mid_player_data = mid_player_data.drop('position', axis=1)\n",
    "# mid_player_data = mid_player_data[mid_cols]\n",
    "\n",
    "# fwd_player_data = player_data_all[player_data_all['position']=='FWD']\n",
    "# fwd_player_data = fwd_player_data.drop('position', axis=1)\n",
    "# fwd_player_data = fwd_player_data[fwd_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run ./goalkeepers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import requests\n",
    "# base_url = \"https://fantasy.premierleague.com/api/\"\n",
    "# general_info = requests.get(base_url + \"/fixtures/?event=12\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>second_name</th>\n",
       "      <th>total_points</th>\n",
       "      <th>form</th>\n",
       "      <th>ict_index</th>\n",
       "      <th>ep_next</th>\n",
       "      <th>event_points</th>\n",
       "      <th>points_per_game</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fábio</td>\n",
       "      <td>Ferreira Vieira</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gabriel</td>\n",
       "      <td>Fernando de Jesus</td>\n",
       "      <td>7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>16.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gabriel</td>\n",
       "      <td>dos Santos Magalhães</td>\n",
       "      <td>51</td>\n",
       "      <td>2.8</td>\n",
       "      <td>51.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kai</td>\n",
       "      <td>Havertz</td>\n",
       "      <td>46</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Karl</td>\n",
       "      <td>Hein</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>Bastien</td>\n",
       "      <td>Meupiyou</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>André</td>\n",
       "      <td>Trindade da Costa Neto</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7</td>\n",
       "      <td>12.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>Carlos Roberto</td>\n",
       "      <td>Forbs Borges</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>Alfie</td>\n",
       "      <td>Pond</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>Tom</td>\n",
       "      <td>Edozie</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>684 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         first_name             second_name  total_points form ict_index  \\\n",
       "0             Fábio         Ferreira Vieira             0  0.0       0.0   \n",
       "1           Gabriel       Fernando de Jesus             7  0.8      16.2   \n",
       "2           Gabriel    dos Santos Magalhães            51  2.8      51.9   \n",
       "3               Kai                 Havertz            46  1.0      73.1   \n",
       "4              Karl                    Hein             0  0.0       0.0   \n",
       "..              ...                     ...           ...  ...       ...   \n",
       "679         Bastien                Meupiyou             0  0.0       0.0   \n",
       "680           André  Trindade da Costa Neto            10  0.7      12.6   \n",
       "681  Carlos Roberto            Forbs Borges             4  0.0       6.4   \n",
       "682           Alfie                    Pond             1  0.3       0.0   \n",
       "683             Tom                  Edozie             0  0.0       0.0   \n",
       "\n",
       "    ep_next  event_points points_per_game  \n",
       "0       0.0             0             0.0  \n",
       "1       1.8             1             0.7  \n",
       "2       3.8             6             4.2  \n",
       "3       2.0             0             4.2  \n",
       "4       0.0             0             0.0  \n",
       "..      ...           ...             ...  \n",
       "679     0.0             0             0.0  \n",
       "680     0.7             2             1.2  \n",
       "681     0.0             0             0.8  \n",
       "682     0.3             1             1.0  \n",
       "683     1.1             0             0.0  \n",
       "\n",
       "[684 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import requests\n",
    "# import pandas as pd\n",
    "\n",
    "# # Endpoint for player data\n",
    "# url = \"https://fantasy.premierleague.com/api/bootstrap-static/\"\n",
    "\n",
    "# # Fetch data\n",
    "# response = requests.get(url)\n",
    "# data = response.json()\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# players = pd.DataFrame(data['elements'])  # Contains player-specific data\n",
    "\n",
    "# # Display key columns\n",
    "# players[['first_name', 'second_name', 'total_points', 'form', 'ict_index', 'ep_next', 'event_points', 'points_per_game']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['chance_of_playing_next_round', 'chance_of_playing_this_round', 'code',\n",
       "       'cost_change_event', 'cost_change_event_fall', 'cost_change_start',\n",
       "       'cost_change_start_fall', 'dreamteam_count', 'element_type', 'ep_next',\n",
       "       'ep_this', 'event_points', 'first_name', 'form', 'id', 'in_dreamteam',\n",
       "       'news', 'news_added', 'now_cost', 'photo', 'points_per_game',\n",
       "       'second_name', 'selected_by_percent', 'special', 'squad_number',\n",
       "       'status', 'team', 'team_code', 'total_points', 'transfers_in',\n",
       "       'transfers_in_event', 'transfers_out', 'transfers_out_event',\n",
       "       'value_form', 'value_season', 'web_name', 'region', 'minutes',\n",
       "       'goals_scored', 'assists', 'clean_sheets', 'goals_conceded',\n",
       "       'own_goals', 'penalties_saved', 'penalties_missed', 'yellow_cards',\n",
       "       'red_cards', 'saves', 'bonus', 'bps', 'influence', 'creativity',\n",
       "       'threat', 'ict_index', 'starts', 'expected_goals', 'expected_assists',\n",
       "       'expected_goal_involvements', 'expected_goals_conceded',\n",
       "       'influence_rank', 'influence_rank_type', 'creativity_rank',\n",
       "       'creativity_rank_type', 'threat_rank', 'threat_rank_type',\n",
       "       'ict_index_rank', 'ict_index_rank_type',\n",
       "       'corners_and_indirect_freekicks_order',\n",
       "       'corners_and_indirect_freekicks_text', 'direct_freekicks_order',\n",
       "       'direct_freekicks_text', 'penalties_order', 'penalties_text',\n",
       "       'expected_goals_per_90', 'saves_per_90', 'expected_assists_per_90',\n",
       "       'expected_goal_involvements_per_90', 'expected_goals_conceded_per_90',\n",
       "       'goals_conceded_per_90', 'now_cost_rank', 'now_cost_rank_type',\n",
       "       'form_rank', 'form_rank_type', 'points_per_game_rank',\n",
       "       'points_per_game_rank_type', 'selected_rank', 'selected_rank_type',\n",
       "       'starts_per_90', 'clean_sheets_per_90'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# players.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
