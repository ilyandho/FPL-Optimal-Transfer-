{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "# from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor, ColumnTransformer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from xgboost import XGBRegressor as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.pipeline import Pipeline  # Use the imblearn pipeline\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the game week\n",
    "gw = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the linear model\n",
    "def Linear_regression(features_train, features_test, target_train, target_test):\n",
    "    # Before using our data, we need to do feature scaling and we opt for the 'standardization' method of scaling.\n",
    "    # The 'standardization' is avaliable thorugh the StandardScaler() method\n",
    "    # Transformers help in batching tasks in a pipepline. In this case, the data is scaled and then a linear regression model is fitted on the scaled data.\n",
    "    # We use a transformer that takes the regression model and the transformation method\n",
    "    # The TransformedTargetRegressor does the transformation and when we do the prediction, it automatically does the inverse transformation (scaling) and returns the values\n",
    "    bool_cols = features_train.drop(columns=['was_home']).columns.tolist()\n",
    "    # categorical_cols = ['was_home']\n",
    "    bool_cols = features_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_cols = features_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('one_hot_encoder', OneHotEncoder())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, bool_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "        ])\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    model = TransformedTargetRegressor(regressor=pipeline, transformer=StandardScaler())\n",
    "\n",
    "    # TransformedTargetRegressor(\n",
    "    #     LinearRegression(), transformer=StandardScaler())\n",
    "\n",
    "    # fit the transofrmer on the train data\n",
    "    model.fit(features_train, target_train)\n",
    "\n",
    "    # With the model fitted, we can predict the total_points given the feature_train and feature_test set\n",
    "    pred_train = model.predict(features_train)\n",
    "    pred_test = model.predict(features_test)\n",
    "\n",
    "    # Evaluate the performance of the model on both sets using the mean absolute error\n",
    "    train_MAE = mean_absolute_error(target_train, pred_train)\n",
    "    test_MAE = mean_absolute_error(target_test, pred_test)\n",
    "\n",
    "    # Evaluate the performance of the model on both sets using the mean square error\n",
    "    train_MSE = mean_squared_error(target_train, pred_train)\n",
    "    test_MSE = mean_squared_error(target_test, pred_test)\n",
    "\n",
    "    # Evaluate the performance of the model on both sets using the root mean square error\n",
    "    train_RMSE = mean_squared_error(target_train, pred_train, squared=False)\n",
    "    test_RMSE = mean_squared_error(target_test, pred_test, squared=False)\n",
    "\n",
    "    # Get the score of the model or the coeeficient of determination i.e how much of the target value can be explained by the model.\n",
    "    # In this case, 0.6 implies that 60% of the variations in the target value can be explained by the model and 40% is not explainable\n",
    "    R2_train = model.score(features_train, target_train)\n",
    "    R2_test = model.score(features_test, target_test)\n",
    "\n",
    "    # If the test error significantly differs from the train error, then there is either overfitting or underfitting\n",
    "    # RMSE, just like the squared loss function that it derives from, effectively penalizes larger errors more severely.\n",
    "    print('Training set RMSE: {}'.format(train_RMSE))\n",
    "    print('Test set RMSE: {}'.format(test_RMSE))\n",
    "\n",
    "    print('Training set R2: {}'.format(R2_train))\n",
    "    print('Test set R2: {}'.format(R2_test))\n",
    "\n",
    "    # Carry out cross validation of the model.\n",
    "    # The evaluation method is the root mean square error\n",
    "    # The method expects a utility function (greater is better) and so the scoring function is the opposite of the the RMSE. Hence the -ve\n",
    "    tree_rmses = -cross_val_score(model, features_train, target_train,\n",
    "                                  scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "\n",
    "    return {'train_MAE': train_MAE, 'test_MAE': test_MAE, 'train_MSE': train_MSE, 'test_MSE': test_MSE, 'train_RMSE': train_RMSE, 'test_RMSE': test_RMSE, 'cv_rmse': tree_rmses.mean(), 'R2_train': R2_train, 'R2_test': R2_test}\n",
    "\n",
    "# Decision Tree Model\n",
    "def DecisionTreeRegression(features_train, features_test, target_train, target_test):\n",
    "    # The DecisionTreeRegressor is passed as the model to the TransformedTreeRegressor together with the StandardScaler\n",
    "    bool_cols = features_train.drop(columns=['was_home']).columns.tolist()\n",
    "    # categorical_cols = ['was_home']\n",
    "    bool_cols = features_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_cols = features_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('one_hot_encoder', OneHotEncoder())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, bool_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "        ])\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', DecisionTreeRegressor())\n",
    "    ])\n",
    "\n",
    "    model = TransformedTargetRegressor(regressor=pipeline, transformer=StandardScaler())\n",
    "    # TransformedTargetRegressor(\n",
    "    #     DecisionTreeRegressor(), transformer=StandardScaler())\n",
    "    model.fit(features_train, target_train)\n",
    "\n",
    "    pred_train = model.predict(features_train)\n",
    "    pred_test = model.predict(features_test)\n",
    "\n",
    "    train_MAE = mean_absolute_error(target_train, pred_train)\n",
    "    test_MAE = mean_absolute_error(target_test, pred_test)\n",
    "\n",
    "    train_MSE = mean_squared_error(target_train, pred_train)\n",
    "    test_MSE = mean_squared_error(target_test, pred_test)\n",
    "\n",
    "    train_RMSE = mean_squared_error(target_train, pred_train, squared=False)\n",
    "    test_RMSE = mean_squared_error(target_test, pred_test, squared=False)\n",
    "\n",
    "    R2_train = model.score(features_train, target_train)\n",
    "    R2_test = model.score(features_test, target_test)\n",
    "\n",
    "    tree_rmses = -cross_val_score(model, features_train, target_train,\n",
    "                                  scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "\n",
    "    return {'train_MAE': train_MAE, 'test_MAE': test_MAE, 'train_MSE': train_MSE, 'test_MSE': test_MSE,\n",
    "            'train_RMSE': train_RMSE, 'test_RMSE': test_RMSE, 'cv_rmse': tree_rmses.mean(), 'R2_train': R2_train, 'R2_test': R2_test}\n",
    "\n",
    "# RandomForestRegressor\n",
    "def RandomForestRegression(features_train, features_test, target_train, target_test, hyperparameters):\n",
    "    # RandomForestRegressor is an ensemble method\n",
    "    # The TransformedTargetRegressor is passed the RandomForestRegressor model\n",
    "    # The RandomForestRegressor is passed some hyper-parameters such as;\n",
    "    # n_esimtaors: number of trees in the forest,\n",
    "    # max_depth: the maximum depth of the tree,\n",
    "    # criterion: the function to measure the quality of the split\n",
    "\n",
    "    bool_cols = features_train.drop(columns=['was_home']).columns.tolist()\n",
    "    # categorical_cols = ['was_home']\n",
    "    bool_cols = features_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_cols = features_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('one_hot_encoder', OneHotEncoder())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, bool_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "        ])\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', RandomForestRegressor(\n",
    "                        n_estimators=hyperparameters['n_estimators'],\n",
    "                        max_depth=hyperparameters['max_depth'],\n",
    "                        criterion=hyperparameters['criterion'], random_state=18\n",
    "                        ),)\n",
    "    ])\n",
    "\n",
    "    model = TransformedTargetRegressor(regressor=pipeline, transformer=StandardScaler())\n",
    "\n",
    "    # model = TransformedTargetRegressor( transformer=StandardScaler())\n",
    "    model.fit(features_train, target_train)\n",
    "\n",
    "    pred_train = model.predict(features_train)\n",
    "    pred_test = model.predict(features_test)\n",
    "\n",
    "    train_MAE = mean_absolute_error(target_train, pred_train)\n",
    "    test_MAE = mean_absolute_error(target_test, pred_test)\n",
    "\n",
    "    train_MSE = mean_squared_error(target_train, pred_train)\n",
    "    test_MSE = mean_squared_error(target_test, pred_test)\n",
    "\n",
    "    train_RMSE = mean_squared_error(target_train, pred_train, squared=False)\n",
    "    test_RMSE = mean_squared_error(target_test, pred_test, squared=False)\n",
    "\n",
    "    R2_train = model.score(features_train, target_train)\n",
    "    R2_test = model.score(features_test, target_test)\n",
    "\n",
    "    tree_rmses = -cross_val_score(model, features_train, target_train,\n",
    "                                  scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "\n",
    "    return {'train_MAE': train_MAE, 'test_MAE': test_MAE, 'train_MSE': train_MSE, 'test_MSE': test_MSE,\n",
    "            'train_RMSE': train_RMSE, 'test_RMSE': test_RMSE, 'cv_rmse': tree_rmses.mean(), 'R2_train': R2_train, 'R2_test': R2_test}\n",
    "\n",
    "def XGBoostRegression(features_train, features_test, target_train, target_test, hyperparameters):\n",
    "\n",
    "    bool_cols = features_train.drop(columns=['was_home']).columns.tolist()\n",
    "    # categorical_cols = ['was_home']\n",
    "    bool_cols = features_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_cols = features_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('one_hot_encoder', OneHotEncoder())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, bool_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "        ])\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', xgb(learning_rate=hyperparameters[\"learning_rate\"],\n",
    "                    n_estimators=hyperparameters[\"n_estimators\"],\n",
    "                    max_depth=hyperparameters[\"max_depth\"],\n",
    "                    eval_metric='rmsle'),)\n",
    "    ])\n",
    "\n",
    "    model = TransformedTargetRegressor(regressor=pipeline, transformer=StandardScaler())\n",
    "\n",
    "    model.fit(features_train, target_train)\n",
    "\n",
    "    # =========================================================================\n",
    "    # To use early_stopping_rounds:\n",
    "    # \"Validation metric needs to improve at least once in every\n",
    "    # early_stopping_rounds round(s) to continue training.\"\n",
    "    # =========================================================================\n",
    "    # first perform a test/train split\n",
    "    # from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # X_train,X_test,y_train,y_test = train_test_split(X_train,y_train, test_size = 0.2)\n",
    "    # model.fit(X_train, y_train, early_stopping_rounds=6, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "    # =========================================================================\n",
    "    # use the model to predict the prices for the test data\n",
    "    # =========================================================================\n",
    "    # predictions = model.predict(goalkeepers_splits['feature_test'])\n",
    "\n",
    "    pred_train = model.predict(features_train)\n",
    "    pred_test = model.predict(features_test)\n",
    "\n",
    "    train_MAE = mean_absolute_error(target_train, pred_train)\n",
    "    test_MAE = mean_absolute_error(target_test, pred_test)\n",
    "\n",
    "    train_MSE = mean_squared_error(target_train, pred_train)\n",
    "    test_MSE = mean_squared_error(target_test, pred_test)\n",
    "\n",
    "    train_RMSE = mean_squared_error(target_train, pred_train, squared=False)\n",
    "    test_RMSE = mean_squared_error(target_test, pred_test, squared=False)\n",
    "\n",
    "    R2_train = model.score(features_train, target_train)\n",
    "    R2_test = model.score(features_test, target_test)\n",
    "\n",
    "    tree_rmses = -cross_val_score(model, features_train, target_train,\n",
    "                                  scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "\n",
    "    return {'train_MAE': train_MAE, 'test_MAE': test_MAE, 'train_MSE': train_MSE, 'test_MSE': test_MSE,\n",
    "            'train_RMSE': train_RMSE, 'test_RMSE': test_RMSE, 'cv_rmse': tree_rmses.mean(), 'R2_train': R2_train, 'R2_test': R2_test}\n",
    "\n",
    "def Logistic_regression(features_train, features_test, target_train, target_test):\n",
    "    encoder = LabelEncoder()\n",
    "    cs_train_ = encoder.fit_transform(target_train)\n",
    "    cs_test_ = encoder.transform(target_test)\n",
    "\n",
    "    # bool_cols = feats_train.drop(columns=['was_home']).columns.tolist()\n",
    "    # categorical_cols = ['was_home']\n",
    "    bool_cols = feats_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_cols = feats_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('one_hot_encoder', OneHotEncoder(sparse=False)),\n",
    "        # ('to_dense', ToDense())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, bool_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "        ])\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('smote', BorderlineSMOTE(sampling_strategy='minority', random_state=42)),\n",
    "        ('model', LogisticRegression(class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(features_train, cs_train_)\n",
    "\n",
    "    train_score = pipeline.score(features_train, cs_train_)\n",
    "    test_score = pipeline.score(features_test, cs_test_)\n",
    "    # Make predictions on the test set\n",
    "    cs_pred = pipeline.predict(features_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy =  accuracy_score(cs_test_, cs_pred)\n",
    "\n",
    "    conf_mat = confusion_matrix(cs_test_,cs_pred)\n",
    "\n",
    "    class_report = classification_report(cs_test_, cs_pred)\n",
    "\n",
    "    unique, counts = np.unique(cs_test_, return_counts=True)\n",
    "    print(\"Class distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "\n",
    "    return {'train_score': train_score, 'test_score': test_score, 'accuracy': accuracy, 'conf_mat': conf_mat, 'class_report': class_report}\n",
    "\n",
    "def Random_Forest_Classifier(features_train, features_test, target_train, target_test):\n",
    "    encoder = LabelEncoder()\n",
    "    cs_train_ = encoder.fit_transform(target_train)\n",
    "    cs_test_ = encoder.transform(target_test)\n",
    "\n",
    "    # bool_cols = feats_train.drop(columns=['was_home']).columns.tolist()\n",
    "    # categorical_cols = ['was_home']\n",
    "    bool_cols = feats_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_cols = feats_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('one_hot_encoder', OneHotEncoder(sparse=False)),\n",
    "        # ('to_dense', ToDense())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, bool_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "        ])\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('smote', BorderlineSMOTE(sampling_strategy='auto', random_state=42)),  # Apply SMOTE to the data\n",
    "        ('classifier', RandomForestClassifier(bootstrap = False, max_depth= None, max_features= 'sqrt', min_samples_leaf = 1, min_samples_split = 2, n_estimators = 100 ,class_weight='balanced', random_state=42))  # Random Forest Classifier\n",
    "    ])\n",
    "\n",
    "\n",
    "    pipeline.fit(features_train, cs_train_)\n",
    "\n",
    "    train_score = pipeline.score(features_train, cs_train_)\n",
    "    test_score = pipeline.score(features_test, cs_test_)\n",
    "    # Make predictions on the test set\n",
    "    cs_pred = pipeline.predict(features_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy =  accuracy_score(cs_test_, cs_pred)\n",
    "\n",
    "    conf_mat = confusion_matrix(cs_test_,cs_pred)\n",
    "\n",
    "    class_report = classification_report(cs_test_, cs_pred)\n",
    "\n",
    "    unique, counts = np.unique(cs_test_, return_counts=True)\n",
    "\n",
    "\n",
    "    return {'train_score': train_score, 'test_score': test_score, 'accuracy': accuracy, 'conf_mat': conf_mat, 'class_report': class_report}\n",
    "\n",
    "def GridSearchParams(features_train, target_train):\n",
    "    # Instatiate the model\n",
    "    model = RandomForestRegressor()\n",
    "\n",
    "    param_grid = {'n_estimators': [8, 10, 12, 14, 16, 18, 20]}\n",
    "\n",
    "    # Define the possible values of the hyperparameter\n",
    "    grid = {\n",
    "        'n_estimators': [8, 10, 12, 14, 16, 18, 20, 200, 300, 400, 500],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'max_depth': [4, 5, 6, 7, 8],\n",
    "        'criterion': ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'],\n",
    "        'random_state': [18]\n",
    "    }\n",
    "\n",
    "    # Deine the model with cv=3 for a 3-fold cross validation\n",
    "    # GridSearchCV has the best_estimator_ parameter that returns the  estimator\n",
    "    # which gave highest score (or smallest loss if specified)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        model, grid, cv=3, scoring='neg_root_mean_squared_error')\n",
    "    grid_search.fit(features_train, target_train)\n",
    "\n",
    "    # Get the best param combination\n",
    "    print(grid_search.best_estimator_)\n",
    "\n",
    "    return {'train_RMSE': train_RMSE, 'test_RMSE': test_RMSE, 'R2_train': R2_train, 'R2_test': R2_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a function that splits and returns features_train, features_test, target_train, target_test\n",
    "\n",
    "def split_data(data):\n",
    "    # Store the 'total_points' target in the 'player_target' variable\n",
    "    # and the rest in the player_features variable\n",
    "    player_target = data['pts_bps']\n",
    "    player_features = data.drop(\"pts_bps\", axis=1)\n",
    "\n",
    "    # The train_test_split function splits the set into train and test sets while maintain the same data distribution over both sets.\n",
    "    # It takes the feature and target sets and reutrns the respective train and test sets\n",
    "    features_train, features_test, target_train, target_test = train_test_split(\n",
    "        player_features, player_target, test_size=0.2)\n",
    "\n",
    "    return {'feature_train': features_train, 'features_test': features_test, 'target_train': target_train, 'target_test': target_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_22_23 = pd.read_csv('./data/joint/22-23/merged_player_data.csv').dropna()\n",
    "data_23_24 = pd.read_csv('./data/joint/23-24/merged_player_data.csv').dropna()\n",
    "data_24_25 = pd.read_csv('./data/joint/24-25/merged_player_data.csv')\n",
    "data_tar = data_24_25[data_24_25['event']==gw]\n",
    "data_24_25_ = data_24_25[data_24_25['event'] != gw]\n",
    "\n",
    "data = pd.concat([data_22_23, data_23_24, data_24_25_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert player positions for current gwk\n",
    "data_tar_ = data_tar.copy()\n",
    "\n",
    "def convert_position(row):\n",
    "    mapping = {'1': 'GK', '2': 'DEF', '3': 'MID', '4': 'FWD'}\n",
    "    return mapping.get(row, None)  # Convert to string before lookup\n",
    "\n",
    "data_tar_['position'] = data_tar_['position'].apply(convert_position)\n",
    "data_tar_['id'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    Total Points – Bonus Points (tp-bp), Minutes, Yellow Cards, Red Cards, Expected Goals (xG), Expected Assists (xA), Non-penalty Expected Goals (npxG),\n",
    "    Shots, Expected Goals Against, _Expected_goal_involvements_,  clean_sheets, ict_index, opponent_team, Expected Goals Buildup (xG Buildup), threat, value,\n",
    "    Key Passes,\n",
    "\n",
    "\n",
    "    _Games_,  Expected Goals Chain (xG Chain),  _Non-penalty Expected Goal Difference (npxGD)_, _Non-penalty Expected Goals Against (npxGA)_, Expected Points (xPts)\n",
    "```\n",
    "\n",
    "```js\n",
    "    Total Points – Bonus Points (tp-bp)\tfor, mid, def, gk\n",
    "    Minutes\tfor, mid, def, gk\n",
    "    Yellow Cards\tfor, mid, def, gk\n",
    "    Red Cards\tfor, mid, def, gk\n",
    "    Expected Goals (xG)\tfor, mid, def, gk\n",
    "    Expected Assists (xA)\tfor, mid, def, gk\n",
    "    Non-penalty Expected Goals (npxG)\tfor, mid, def\n",
    "    Games\tfor, mid, def, gk\n",
    "    Shots\tfor, mid, def\n",
    "    Key Passes\tfor, mid, def, gk\n",
    "    Expected Goals Chain (xG Chain)\tfor, mid, def\n",
    "    Expected Goals Buildup (xG Buildup)\tfor, mid, def\n",
    "    Non-penalty Expected Goal Difference (npxGD)\tdef, gk\n",
    "    Expected Goals Against\tdef, gk\n",
    "    Non-penalty Expected Goals Against (npxGA)\tdef, gk\n",
    "    Expected Points (xPts)\tdef, gk\n",
    "    expected_goal_involvements\tfor, mid, def, gk\n",
    "    clean_sheets\tmid, def, gk\n",
    "    ict_index\tfor, mid, def, gk\n",
    "    opponent_team\tfor, mid, def, gk\n",
    "    threat\tfor, mid, def, gk\n",
    "    value\tfor, mid, def, gk\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
